// Traditional RNN

var rnn = function(name, dim, maybeArgs) {
  assert.ok(name, 'A network must be given a name');
  assert.ok(Number.isInteger(dim), 'dim should be an integer');
  var args = maybeArgs || {};
  var ctor = args.ctor || affine;
  var output = args.output || tanh;
  var net = stack([output, ctor(name, dim, maybeArgs), concat]);
  return function(hprev, x) {
    assert.ok(dims(hprev)[0] === dim,
              'Previous hidden vector has unexpected dimension');
    return net([hprev, x]);
  };
};

// Gated Recurrent Unit

// This is similar to the variant described in "Empirical Evaluation
// of Gated Recurrent Neural Networks on Sequence Modeling", which
// computes the candidate activation in a slightly different way from
// the original paper.

// https://arxiv.org/abs/1412.3555

var gru = function(name, dim, maybeArgs) {
  assert.ok(name, 'A network must be given a name');
  assert.ok(Number.isInteger(dim), 'dim should be an integer');
  var args = maybeArgs || {};
  var ctor = args.ctor || affine;
  var update = compose(sigmoid, ctor(name + 'update', dim, maybeArgs));
  var reset = compose(sigmoid, ctor(name + 'reset', dim, maybeArgs));
  var candidate = compose(tanh, ctor(name + 'candidate', dim, maybeArgs));
  return function(hprev, x) {
    assert.ok(dims(hprev)[0] === dim,
              'Previous hidden vector has unexpected dimension');
    var hprevx = concat([hprev, x]);
    var r = reset(hprevx);
    var z = update(hprevx);
    var cand = candidate(concat([T.mul(hprev, r), x]));
    var oneminusz = T.add(T.neg(z), 1);
    return T.add(T.mul(oneminusz, hprev), T.mul(z, cand));
  };
};

// Long Short Term Memory

// This is similar to the variant described in "Generating sequences
// with recurrent neural networks" (Graves 2013). The difference is
// that here there are no 'peep-hole' connections. i.e. The previous
// memory state is not (currently) passed as input to the forget,
// input, output gates.

// https://arxiv.org/abs/1308.0850

var lstm = function(name, dim, maybeArgs) {
  assert.ok(name, 'A network must be given a name');
  // dim is the total dimension of the state. i.e. memory + hidden
  // state vectors. Setting things up this way makes it easy to swap
  // between gru and lstm.
  assert.ok(Number.isInteger(dim), 'dim should be an integer');
  assert.ok(dim % 2 === 0, 'dim should be an even integer');
  var hdim = dim / 2;
  // It's said that initializing the biases of the forget gate to a
  // value greater than 0 is a good idea. This is so that the output
  // is close to one at the start of optimization, ensuring
  // information is passed along. This is mentioned in e.g. "An
  // Empirical Exploration of Recurrent Network Architectures".
  var forget = compose(
    sigmoid,
    affine(name + 'forget', hdim, _.assign({}, maybeArgs, {initb: 1})));
  var input = compose(sigmoid, affine(name + 'input', hdim, maybeArgs));
  var output = compose(sigmoid, affine(name + 'output', hdim, maybeArgs));
  var candidate = compose(tanh, affine(name + 'candidate', hdim, maybeArgs));
  return function(prev, x) {
    // For compatibility with the interface of e.g. gru we combine the
    // memory and hidden state into a single vector, prev.
    assert.ok(dims(prev)[0] === dim,
              'Previous state vector has unexpected dimension');
    var cprev = T.reshape(T.range(prev, 0, hdim), [hdim, 1]);
    var hprev = T.reshape(T.range(prev, hdim, dim), [hdim, 1]);
    var hprevx = concat([hprev, x]);
    var f = forget(hprevx);
    var i = input(hprevx);
    var o = output(hprevx);
    var cand = candidate(hprevx);
    var c = T.add(T.mul(f, cprev), T.mul(i, cand));
    var h = T.mul(o, tanh(c));
    return concat([c, h]);
  };
};
