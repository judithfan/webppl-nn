// Like mem, but cache key is only based on the first argument of the
// memoized function.
var mem1 = function(f) {
  globalStore.mem1Index = 1 + (globalStore.mem1Index || 0);
  var key = 'mem1-' + globalStore.mem1Index;
  return function(arg) {
    var stringedArg = key + util.serialize(arg);
    if (_.has(globalStore, stringedArg)) {
      return globalStore[stringedArg];
    } else {
      var val = apply(f, arguments);
      globalStore[stringedArg] = val;
      return val;
    }
  };
};

// This is a first attempt at providing a method for running bits of
// optimized programs. I think we can do more/better though.

// Calling the regular function `fn` evaluates the function, sampling
// from the target. Calling the result of usingParams(fn) evaluates
// the function sampling from the guide.

var usingGuide = function(fn) {
  return function() {
    var args = arguments;
    sample(Infer({method: 'forward', samples: 1, guide: true, model() {
      return ad.valueRec(apply(fn, args));
    }}));
  };
};

var checkNetName = function(name) {
  return webpplNn.checkNetName(name);
};

// ==================================================
// Model Parameters
// ==================================================

// By model parameter I mean a guide parameter that is included in a
// model by adding a prior distribution that is guided by a delta
// distribution, parameterized by the guide parameter. When optimizing
// the ELBO we can view optimization as performing maximum likelihood
// with regularization on the model parameters.

// WebPPL already has `modelParam` which does this for the case where
// the prior is an improper uniform over the reals. (This improper
// prior corresponded to performing no regularizing.)

// OBSERVATION: If we want model parameters to behave similarly to
// `param`, then fetching a parameter with a given name multiple times
// during the evaluation a model, should always return the same value.
// When optimizing the ELBO, this is automatically the case, since the
// guide is just a Delta on the parameter, and `param` will return the
// same value for a given name. However, if we were to follow the
// current implementation of `modelParam`, this wouldn't be the case
// for other algorithms. Imagine using HMC to do inference for a
// Bayesian neural network, and further that this network is used in
// more than one place in the model, where the sharing comes from the
// re-use of parameter names. Because we're no longer sampling from
// the guide, there is now nothing to induce sharing by name -- each
// call to `modelParam` will sample fresh parameters for the network
// from the prior. This consideration suggests the we ought to cache
// calls to `modelParam`. (Using the parameter name as the cache key.)
// This also makes sense from another perspective -- without such
// caching, every additional call to `modelParam` will add its own
// regularization term to the objective when optimizing the ELBO.

// This helper implements the pattern just described:

var __parameterModel = mem1(function(name, paramOpts, getPrior) {
  // getPrior is a function that returns a distribution instance. It
  // is passed the options that are passed to `param` to make it
  // possible to have the dimension of the prior match that of the
  // guide parameter.
  return sample(getPrior(paramOpts), {guide() {
    return Delta({v: param(paramOpts)});
  }});
});

var parameterModel = function(getPrior) {
  return function(paramOpts) {
    // TODO: Handle address based names.
    assert.ok(paramOpts.name, 'A name must be given.');
    return __parameterModel(paramOpts.name, paramOpts, getPrior);
  };
};

// Note that since it's not possible to sample from an improper prior,
// that fact that the result of sampling isn't cached (as discused
// above) is not a problem.

// (Though without caching, we'll unnecessarily create lots of
// unnecessary AD nodes. I saw this cause "Maximum call stack size
// exceeded" when optimizing a DMM, so adding caching would be a good
// idea.)

// var modelParam = parameterModel(constF(ImproperUniform()));

// The function returned by e.g. modelParamL2(0.1) is analogous the
// WebPPL's `modelParam`.
var modelParamL2 = function(sigma) {
  return parameterModel(function(paramOpts) {
    var dims = paramOpts.dims;
    return dims ?
        TensorGaussian({mu: 0, sigma, dims}) :
        Gaussian({mu: 0, sigma});
  });
};

// TODO: One problem here is that one would expect `modelParam` to
// throw an error in the guide because it calls `sample`. However, if
// the value of the parameter has been cached, it's possible to call
// `modelParam` and successfully fetch the value. This is confusing.
// Perhaps `modelParam` should check whether its called from the
// guide, and error out if so.

// ==================================================
// Neural Networks
// ==================================================

var dims = function(x) {
  return ad.value(x).dims;
};

var concat = function(arr) {
  var t = T.concat(arr);
  return T.reshape(t, [dims(t)[0], 1]);
};

var idMatrix = function(n) {
  return webpplNn.idMatrix(n);
};

var oneHot = function(index, length) {
  return webpplNn.oneHot(index, length);
};

var softplus = function(x) {
  return T.log(T.add(T.exp(x), 1));
};

var softmax = function(x) {
  return T.softmax(x);
};

var squishToProbSimplex = function(x) {
  return dists.squishToProbSimplex(x);
};

var sigmoid = function(x) {
  return T.sigmoid(x);
};

var tanh = function(x) {
  return T.tanh(x);
};

var relu = function(x) {
  return webpplNn.relu(x);
};

var lrelu = function(x) {
  return webpplNn.lrelu(x);
};

// Compose several functions in right to left order.
// stack([f, g, h]) == compose(f, compose(g, h))
var stack = function(arr) {
  return reduce(compose, idF, arr);
};

// When a network is used in the model we often want to include a
// prior over the network's parameters. (So that we're specifying
// fully probabilistic model that could in principle be used with any
// inference algorithm.)

// In order that neural network helpers can be reused in both the
// model and the guide, we need to abstract out the specification of
// the prior and its guide distribution.

// This is done using the model parameter scheme described above.
// Specifically, the neural network constructors take as an argument
// the function that is used to create/fetch parameters. If this
// function is `param` then we end up with a net suitable for use in
// the guide. Alternatively a parameter model function can be passed
// to create a net suitable for use in the model.

// TODO: Write about the problems that arise in taking this approach.

// The biggest problem I see with this approach is inherited from
// `param`. I'll describe the problem with that, the problem with nets
// is similar. A parameter is created/fetched with something like
// `param({name, dims, init})`. Note, we can induce parameter sharing
// by reusing a name more than once. The problem is that when sharing
// is across two or more distinct locations in source code, then there
// is no longer a single canonical location in which dims (and other
// properties) are specified. (The dims from the `param` call
// encountered first will be used.) At best this is an annoyance
// (because the easiest way to cope is to use the same dims at every
// occurrence of a name), but I suspect that in practice it will be
// worse than annoying. A bug arising as a result of a tensor getting
// unexpected dims by virtue of a definition somewhere else in the
// program will, I suspect, cause much confusion. (Especially if the
// sharing by name is accidental, or worse, caused by loading a
// package!) I think this problem stems from the fact that `param`
// smushes together both creation and retrieval of parameters.
// Splitting apart these two operations would offer a number of ways
// of addressing this I think.


// Nets created with "linear" use a weight initialization based on
// "Understanding the difficulty of training deep feedforward neural
// networks", often called Xavier initialization.

// TODO: This initialization scheme may not be suitable for asymmetric
// non-linearities.

var linear = function(name, nout, maybeArgs) {
  checkNetName(name);
  assert.ok(Number.isInteger(nout), 'nout should be an integer.');
  var args = maybeArgs || {};
  var nnparam = args.param || param;
  return function(x) {
    var nin = dims(x)[0];
    var sigma = Math.sqrt(2 / (nin + nout));
    var w = nnparam({name, dims: [nout, nin], sigma});
    return T.dot(w, x);
  };
};

// TODO: It seems pretty common to not apply regularization to biases.
// Is that important? What would it look like here?

var bias = function(name, maybeArgs) {
  checkNetName(name);
  var args = maybeArgs || {};
  var nnparam = args.param || param;
  var initb = _.has(args, 'initb') ? args.initb : 0;
  assert.ok(_.isNumber(initb), 'Initial bias should be a number.');
  return function(x) {
    var nout = dims(x)[0];
    var b = nnparam({name, dims: [nout, 1], mu: initb, sigma: 0});
    return T.add(x, b);
  };
};

var affine = function(name, nout, maybeArgs) {
  checkNetName(name);
  return compose(
    bias(name + 'b', maybeArgs),
    linear(name + 'w', nout, maybeArgs));
};
