// Depends on:
// WebPPL 0.9.7
// https://github.com/null-a/webppl-fs

// Run with:
// webppl --require . --require webppl-fs examples/mnist.wppl

var y = JSON.parse(fs.read('examples/data/mnist_images.json'));
var images = map(Vector, y);
var labels = JSON.parse(fs.read('examples/data/mnist_labels.json'));
var data = map2(function(image, label) { return {image, label}; }, images, labels);
var train = data.slice(0, 50000);
var valid = data.slice(50000);

var argmax = function(vector) {
  var arr = ad.value(vector).toFlatArray();
  var pairs = _.zip(arr, _.range(arr.length));
  return maxWith(first, pairs)[0][1];
};

var makeClassifierNet = function(nnparam) {
  return stack([
    softmax,
    affine(10, 'l2', nnparam),
    tanh,
    affine(100, 'l1', nnparam)
  ]);
};

var classifierNet = makeClassifierNet(param);

// Hack to create a version of the classifier net with fixed params.
// i.e A net where with parameters that are considered to be constant
// by `Optimize`. (Issue 658.)
var unliftParam = function(arg) { return ad.value(param(arg)); };
var classifierNetFixed = makeClassifierNet(unliftParam);

var classify = function(image) {
  return argmax(classifierNet(image));
};

// --------------------------------------------------
// Standard MLP classifier learning.
// --------------------------------------------------
Optimize({
  steps: 500,
  optMethod: {adam: {stepSize: 0.001}},
  model() {
    mapData({data: train, batchSize: 50}, function(x) {
      var ps = classifierNet(x.image);
      observe(Discrete({ps}), x.label);
    });
  }
});

// Measure accuracy on validation set.
// (Achieves around 90% with hidden layer of size 100, taking 500
// gradient steps.)
var accuracy = listMean(map(function(x) {
  return x.label === classify(x.image);
}, valid));
display('Classification accuracy: ' + accuracy);

// --------------------------------------------------
// Image synthesis.
// --------------------------------------------------
var targetClass = 3;

// Optimization. (Treat the image as a model parameter and optimize.)

// var marginal = Infer({
//   method: 'optimize',
//   steps: 200,
//   optMethod: {adam: {stepSize: 0.1}},
//   samples: 1,
//   verbose: true,
//   model() {
//     var image = sigmoid(param({name: 'image', dims: [28*28, 1]}));
//     var ps = classifierNetFixed(image); // Treat net parameters as constants.
//     observe(Discrete({ps}), targetClass);
//     return image;
//   }
// });

// NOTE: In order have inference sample images that have a high
// probability of been from the target class I had to scale up the log
// likelihood in the examples that follow.

// Variational inference.

// var marginal = Infer({
//   method: 'optimize',
//   steps: 200,
//   optMethod: {adam: {stepSize: 0.1}},
//   samples: 10,
//   verbose: true,
//   model() {
//     //var image = Vector(repeat(28*28, function() { return uniform(0,1); }));
//     // Prior on unit interval with most of the mass close to 0 or 1.
//     var image = sigmoid(tensorGaussian({mu: 0, sigma: 5, dims: [28*28, 1]}));
//     var ps = classifierNetFixed(image); // Treat net parameters as constants.
//     factor(Discrete({ps}).score(targetClass) * 100);
//     return image;
//   }
// });

// HMC

var marginal = Infer({
  method: 'MCMC',
  kernel: {HMC: {stepSize: .1, steps: 10}},
  samples: 100,
  verbose: true,
  model() {
    var image = sigmoid(tensorGaussian({mu: 0, sigma: 5, dims: [28*28, 1]}));
    var ps = classifierNet(image); // No need to fix parameters here under HMC.
    factor(Discrete({ps}).score(targetClass) * 100);
    return image;
  }
});


var samples = _.map(marginal.samples, 'value');
var targetClassProbs = map(function(sample) {
  return T.get(classifierNet(sample), targetClass);
}, samples);

display('Probability of target class for sampled images:');
display(targetClassProbs);


// NOTE: The images generated by inverting the classifier don't look
// like digits.

// Vizualize with:
// python examples/data/show_digit.py image.json
//fs.write('image.json', JSON.stringify(last(samples).toFlatArray()));
