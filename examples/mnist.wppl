// Depends on:
// WebPPL 0.9.7
// https://github.com/null-a/webppl-fs

// Run with:
// webppl --require . --require webppl-fs examples/mnist.wppl

// Load VAE parameters.
setParams(deserializeParams(fs.read('vae-params-zdim-10-steps-500.json')));

var y = JSON.parse(fs.read('examples/data/mnist_images.json'));
var images = map(Vector, y);
var labels = JSON.parse(fs.read('examples/data/mnist_labels.json'));
var data = map2(function(image, label) { return {image, label}; }, images, labels);
var train = data.slice(0, 50000);
var valid = data.slice(50000);

var argmax = function(vector) {
  var arr = ad.value(vector).toFlatArray();
  var pairs = _.zip(arr, _.range(arr.length));
  return maxWith(first, pairs)[0][1];
};

var makeClassifierNet = function(nnparam) {
  return stack([
    softmax,
    affine(10, 'l2', nnparam),
    tanh,
    affine(100, 'l1', nnparam)
  ]);
};

// Similar to makeClassifierNet but written without webppl-nn. (Only
// missing weight initialization.)
var makeClassifierNet2 = function(nnparam) {
  return function(x) {
    var w0 = nnparam({name: 'w0', dims: [100, 28*28]});
    var b0 = nnparam({name: 'b0', dims: [100, 1]});
    var w1 = nnparam({name: 'w1', dims: [10, 100]});
    var b1 = nnparam({name: 'b1', dims: [10]});
    var a0 = T.add(T.dot(w0, x), b0);
    var h0 = T.tanh(a0);
    var a1 = T.add(T.dot(w1, h0), b1);
    return T.softmax(a1);
  };
};

var classifierNet = makeClassifierNet(param);

// Hack to create a version of the classifier net with fixed params.
// i.e A net where with parameters that are considered to be constant
// by `Optimize`. (Issue 658.)
var unliftParam = function(arg) { return ad.value(param(arg)); };
var classifierNetFixed = makeClassifierNet(unliftParam);

var classify = function(image) {
  return argmax(classifierNet(image));
};

// --------------------------------------------------
// Standard MLP classifier learning.
// --------------------------------------------------
Optimize({
  steps: 500,
  optMethod: {adam: {stepSize: 0.001}},
  model() {
    mapData({data: train, batchSize: 50}, function(x) {
      var ps = classifierNet(x.image);
      observe(Discrete({ps}), x.label);
    });
  }
});

// Measure accuracy on validation set.
// (Achieves around 90% with hidden layer of size 100, taking 500
// gradient steps.)
var accuracy = listMean(map(function(x) {
  return x.label === classify(x.image);
}, valid));
display('Classification accuracy: ' + accuracy);

// --------------------------------------------------
// Image synthesis.
// --------------------------------------------------
var targetClass = 7;

// Optimization. (Treat the image as a model parameter and optimize.)

// var marginal = Infer({
//   method: 'optimize',
//   steps: 200,
//   optMethod: {adam: {stepSize: 0.1}},
//   samples: 1,
//   verbose: true,
//   model() {
//     var image = sigmoid(param({name: 'image', dims: [28*28, 1]}));
//     var ps = classifierNetFixed(image); // Treat net parameters as constants.
//     observe(Discrete({ps}), targetClass);
//     return image;
//   }
// });

// NOTE: In order have inference sample images that have a high
// probability of been from the target class I had to scale up the log
// likelihood in the examples that follow.

// Variational inference.

// var marginal = Infer({
//   method: 'optimize',
//   steps: 200,
//   optMethod: {adam: {stepSize: 0.1}},
//   samples: 10,
//   verbose: true,
//   model() {
//     //var image = Vector(repeat(28*28, function() { return uniform(0,1); }));
//     // Prior on unit interval with most of the mass close to 0 or 1.
//     var image = sigmoid(tensorGaussian({mu: 0, sigma: 5, dims: [28*28, 1]}));
//     var ps = classifierNetFixed(image); // Treat net parameters as constants.
//     factor(Discrete({ps}).score(targetClass) * 100);
//     return image;
//   }
// });

// HMC

// var marginal = Infer({
//   method: 'MCMC',
//   kernel: {HMC: {stepSize: .1, steps: 10}},
//   samples: 10,
//   verbose: true,
//   model() {
//     var image = sigmoid(tensorGaussian({mu: 0, sigma: 5, dims: [28*28, 1]}));
//     var ps = classifierNet(image); // No need to fix parameters here under HMC.
//     factor(Discrete({ps}).score(targetClass) * 100);
//     return image;
//   }
// });




// Using the VAE generative model as prior.

// The following need to match the settings used when generating the
// saved parameters:
var zDim = 10;
var hDecodeDim = 500;
var xDim = 784;

var decode = stack([
  sigmoid,
  affine(xDim, 'dec1', unliftParam),
  tanh,
  affine(hDecodeDim, 'dec0', unliftParam)
]);

var marginal = Infer({
  method: 'MCMC',
  // We might benefit from tuning this some:
  kernel: {HMC: {stepSize: .05, steps: 10}},
  samples: 36,
  lag: 10,
  burn: 200,
  verbose: true,
  model() {
    var z = tensorGaussian({mu: 0, sigma: 1, dims: [zDim, 1]});
    var image = decode(z);
    var ps = classifierNetFixed(image);
    // Still scaling the likelihood...
    factor(Discrete({ps}).score(targetClass) * 100);
    return image;
  }
});

var samples = _.map(marginal.samples, 'value');
var targetClassProbs = map(function(sample) {
  return T.get(classifierNet(sample), targetClass);
}, samples);

display('Probability of target class for sampled images:');
display(targetClassProbs);


// Vizualize with:
// python examples/data/show_digits.py images.json
var tensorToArray = function(tensor) {
  return tensor.toFlatArray();
};
fs.write('images.json', JSON.stringify(map(tensorToArray, samples)));
