// Structured Inference Networks for Nonlinear State Space Models
// https://arxiv.org/abs/1609.09869

// Requires the version of webppl available as pull request #745.

// Run with:
// webppl examples/dmm.wppl --require .

// Delta between this and the model described in the paper:

// 1. We don't compute any KL terms analytically.
// 2. We parameterize sigma with our nets, rather than sigma^2.
// 3. No regularization. (?)
// 4. No dropout in RNN. (?)
// 5. Weight init. e.g. ortho for LSTM, what else?
// 6. Step sizes are probably not comparable because ML people usually
// divide mini-batch gradients by the batch size, where as we
// effectively multiple them by len(data)/batchSize.
// 7. We don't initialize the weight matrix of the linear network
// (called muNet here) that forms part of the transition function in
// quite the same way.

// HELPERS -----------------------------------------------------------

var reduceLeft = function(f, init, arr) {
  // if (arr.length > 0) {
  //   return reduceLeft(f, f(init, arr[0]), rest(arr));
  // } else {
  //   return init;
  // }
  var helper = function(i, init) {
    if (i < arr.length) {
      return helper(i + 1, f(init, arr[i]));
    } else {
      return init;
    }
  };
  return helper(0, init);
};

var loadMusic = function(fn, part) {
  // part should be one of 'train', 'test', 'valid'.
  var data = JSON.parse(fs.read(fn))[part];
  return map(function(seq) { return map(Vector, seq); }, data);
};

// PARAMETERS --------------------------------------------------------

var fullSize = false;

// Appendix C describes the size of the nets for each experiment.
var xDim = 88;
var zDim = fullSize ? 100 : 5;
var transitionDim = fullSize ? 200 : 5;
var emissionDim = fullSize ? 100 : 5;
var rnnDim = fullSize ? 400 : 5; // or 600

// I'm not sure about the dimension of the generative net. The above
// is taken from the paper and I assume it's correct, though the code
// make it look like twice this size is used.

// Also see:
// https://github.com/clinicalml/structuredinference/blob/7fd2e774a8aebe3a946a24a28c4f50d623f64014/parse_args_dkf.py

// MODEL NETS --------------------------------------------------------

// See section 5 for the description of the emission and transition
// functions.

// "gated transition function"
var gNet = stack([
  sigmoid,
  affine(zDim, 'gNetH2', modelParamL2(1)),
  relu,
  affine(transitionDim, 'gNetH1', modelParamL2(1))
]);

var hNet = stack([
  affine(zDim, 'hNetH2', modelParamL2(1)),
  relu,
  affine(transitionDim, 'hNetH1', modelParamL2(1))
]);


//var muNet = affine(zDim, 'muNet', modelParamL2(1));

// Since we don't have the option to init. the weight matrix to the
// identity in webppl-nn, we instead parameterize this as a
// optimizable matrix plus the identity. In the presence of
// regularization, this isn't the same thing, as we regularize towards
// the identity whereas when initializing to the identity we'll still
// regularize towards 0.
var muNetI = idMatrix(zDim);
var muNet = compose(
  bias('muNetb', modelParamL2(1)),
  function(x) {
    var w = modelParamL2(1)({name: 'muNetw', dims: [zDim, zDim]});
    return T.dot(T.add(muNetI, w), x);
  });

var sigmaNet = stack([
  softplus,
  affine(zDim, 'sigmaNetHid', modelParamL2(1)),
  relu
]);

var zDist = function(zPrev) {
  if (zPrev === undefined) {
    return TensorGaussian({mu: 0, sigma: 1, dims: [zDim, 1]});
  }
  else {
    var g = gNet(zPrev);
    var h = hNet(zPrev);
    var mu = T.add(
      T.mul(T.add(T.neg(g), 1), muNet(zPrev)),
      T.mul(g, h));
    var sigma = sigmaNet(h);
    return DiagCovGaussian({mu, sigma});
  }
};

// "emission function"
var emissionNet = stack([
  sigmoid,
  affine(xDim, 'emissionNetH3', modelParamL2(1)),
  relu,
  affine(emissionDim, 'emissionNetH2', modelParamL2(1)),
  relu,
  affine(emissionDim, 'emissionNetH1', modelParamL2(1))
]);

var xDist = function(z) {
  var ps = emissionNet(z);
  return MultivariateBernoulli({ps});
};

// GUIDE NETS --------------------------------------------------------

// See section 4.

// This is the DKS variant described in the paper.

// We need the RNN to consume all of the observations (starting with
// the final observation) before we can output guide parameters for
// the first latent variable. We do that here, storing the RNN state
// in the global store for later use.

// To generate guide parameters we need to know where we are in the
// sequence so we can use the corresponding RNN state generated
// earlier. For now we track this using the global store.

// NOTE: For lstm we double rnnDim so that hidden state and
// memory/context both have length rnnDim. This is what i understand
// the paper to describe.

// TODO: Make this a parameter?
var rnnInitialState = zeros([rnnDim * 2, 1]);
var rnn = lstm(rnnDim * 2, 'rnn');

var init = function(sequence) {
  return function() {
    // Initialize state.
    globalStore.t = 0;
    // Run the RNN.
    globalStore.rnnStates = [];
    reduce(function(obs, prevState) {
      var state = rnn(prevState, obs);
      // Put the new state on the from of the array so that when we're
      // done, the rnn states are in temporal order. I think
      // performace of runRNN will have undesirable asymptotics
      // whether we do this or stick it on the end. Is there a way to
      // avoid this other than judicious use of mutation?
      globalStore.rnnStates = [state].concat(globalStore.rnnStates);
      return state;
    }, rnnInitialState, sequence);
  };
};

// The "combiner function". This is what we've typically called the
// predict net in daipp.

var z0 = zeros([zDim, 1]);

var embedNet = compose(tanh, affine(rnnDim, 'embedNet'));
var muParamNet = affine(zDim, 'muParamNet');
var sigmaParamNet = compose(softplus, affine(zDim, 'sigmaParamNet'));

// The guide at each time step is conditioned on the previous hidden
// state (which we pass in here since we it's already to hand in the
// model) and all future observations. (Via the RNN hidden state,
// which is available via the global store.).

var zGuide = function(zPrev) {
  return function() {
    var t = globalStore.t;
    var state = globalStore.rnnStates[t];
    var hCombined = T.mul(T.add(embedNet(zPrev || z0), state), 0.5);
    var mu = muParamNet(hCombined);
    var sigma = sigmaParamNet(hCombined);
    globalStore.t = t + 1;
    return DiagCovGaussian({mu, sigma});
  };
};

// MODEL -------------------------------------------------------------

var model = function(data) {
  return mapData({data, batchSize: 20}, function(sequence) {
    guide(init(sequence));
    return reduceLeft(function(zPrev, obs) {
      var z = sample(zDist(zPrev), {guide: zGuide(zPrev)});
      observe(xDist(z), obs);
      return z;
    }, undefined, sequence);
  });
};

// After optimization we want to sample from the guide, conditioned on
// some input. To do that we need keep track of all previous z during
// the `reduceLeft`. `obsFn` does this. We could also use this in
// `model`, but it's asymptotically slower than just keeping the most
// recent latent around. (Thought this may not matter much.) I've not
// decided how best to fix this yet. (My proposed fix for the `map`
// relies on allocating the output array ahead of this, but this
// strategy can't work for `reduce` in general, since we don't always
// return an array. Ugh.)

var obsFn = function(sequence) {
  guide(init(sequence));
  return reduceLeft(function(prev, obs) {
    var zPrev = (prev.length > 0) ? last(prev) : undefined;
    var z = sample(zDist(zPrev), {guide: zGuide(zPrev)});
    observe(xDist(z), obs);
    return prev.concat(z);
  }, [], sequence);
};

// samples from q(z|x)
var posterior = usingGuide(obsFn);

// We might also want to generate samples from the model using
// optimized parameters. This is tricky to do with the current
// implementation, since what that amounts to is sampling optimized
// weights for the (model nets) from the guide, but sampling x and z
// from the model. (Is the right? The first `z` would also be drawn
// from TensorGaussian({mu: 0, sigma: 1})?)

// One work-around would be to ditch `modelParam` from model nets,
// then after optimization sampling from the prior would automatically
// use optimized model nets.

// OPTIMIZATION ------------------------------------------------------

// var obs = map(function(x) { return Vector([x]); }, [0,1,1]);
// Infer({method: 'forward', guide: 0, samples: 1, model() { model([obs]); }});

var data = loadMusic('examples/data/music/JSB-chorales.json', 'train');

Optimize({
  model() { return model(data); },
  steps: 1000,
  optMethod: {adam: {stepSize: 0.0008}},
  logProgress: true,
  checkpointParams: true
});
